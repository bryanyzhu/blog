---
title: 利用无标注数据提升语义分割模型
author: 朱毅 Amazon Applied Scientist
---

![](img/self-semseg-author.png){:width="640"}

## 前言

一直以来，分割领域最头疼的问题就是数据标注。当别的领域都已经拥有百万级的标准数据集的时候，语义分割还在玩只有几千张图片的Cityscapes和ADE20K。由此引发的问题有很多，比如刷榜举步维艰，模型的泛化能力差。尤其在自动驾驶领域，一个城市训练的街景语义分割模型，换个城市就没法部署了，这大大限制了语义分割在实际中的应用。

![](img/self-semseg-DAsample.png){:width="720"}

然而分割的标注及其昂贵，标一张图片动辄就要几个小时，所以很难构建一个百万甚至千万级别的数据集。因此*如何利用没有标注的数据来提升模型的性能*就变得非常吸引人。


## [Improving Semantic Segmentation via Self-Training](https://arxiv.org/abs/2004.14960)

于是我们本文提出了一种简单有效的基于teacher-student的自训练框架，

1. 借助于大量的无标注数据，我们在三个标准数据集（Cityscapes, CamVid和KITTI）上*获得了最好成绩*。
2. 在只有极少数据的新场景，甚至有新类别的情况下，*模型依然能通过自训练迅速迁移*。
3. 提出一种新的训练策略，可以在不牺牲分割精度的前提下*提速2倍*。

![](img/self-semseg-overview.png){:width="720"}


## 实验结果

### State-of-the-art 结果

先来看一波性能，在只用Cityscapes三千张训练图片的情况下，我们的模型在Cityscapes的测试集上取得了82.7%的mIoU，跟使用了大量Mapillary标注数据去预训练模型得到的结果是一样的！如果你对语义分割不是很了解，那等价到目标检测领域，这个结果相当于不用COCO数据集去预训练目标检测模型，在Pascal VOC上依旧可以达到和用了COCO预训练一样的结果！

![](img/self-semseg-cs-sota.png){:width="480"}

同时，在我们的自训练框架下，student模型可以灵活选用不同的网络架构，都能轻松涨点。如下表所示，我们的方法可以在不用额外标注数据的情况下，大幅提高baseline性能，甚至还能超越利用额外标注数据预训练的模型。值得一提的是，*我们训练的实时网络FastSCNN达到了72.5% mIoU的性能，远超[原文](https://arxiv.org/abs/1902.04502) 68.6%和[PaddleSeg](https://github.com/PaddlePaddle/PaddleSeg/blob/release/v0.4.0/docs/model_zoo.md) 中提供的69.6%的结果*。

![](img/self-semseg-cs-diff-student.png){:width="480"}

### 更强的泛化结果

我们在这篇论文中还介绍了一种实用的新场景 (setting): 如何把在地点A训练的街景语义分割模型，利用少量地点B的标注数据去部署到地点B。比如文中从Cityscapes泛化到Mapillary的实验，在每一类只取10个标注样本的情况下，我们的自训练方法依旧可以取得34% mIoU的性能，远超常用的模型微调方法。另外我们还在附录部分做了从Cityscapes泛化到BDD100K的实验，在只用200张标注图片的情况下，性能直逼使用7000张图片的效果！这将大大加速语义分割在实际中的应用。

![](img/self-semseg-cs2map.png){:width="480"}

### 训练效率提速2倍

最后我们在文中提出一种普适的模型训练加速的方法，可以在不牺牲精度的情况下提速2倍！基于coarse-to-fine的思想，我们提出了一系列学习策略，通过不停的调整输入图片的大小，从而加速模型训练。比如在Cityscapes上训练一个DeepLabV3+ WideResNet38的模型，一个八卡机器一般需要训练32个小时。利用新的学习策略，训练时间可以缩短到16个小时还不掉点。如果再加上混精度训练，可以进一步缩短到7个小时！早上跑实验，下午就可以看到结果。

![](img/self-semseg-fast-training.png){:width="640"}

更多细节可以参考[论文](https://arxiv.org/abs/2004.14960)。代码在整理好后会和模型一起放出。同时，我们也会把提出的新场景中Mapillary和BDD100K的具体数据划分分享出来，提供一个公平比较的基准。


## 后记

语义分割领域一直都推崇网络架构的改进，很少关注数据方面所能带来的提升。这篇论文是我们在语义分割上对无标注数据利用的一次探索，发现不论在精度还是泛化能力上，模型都能得到很大的提高。类似的想法还可以很自然的拓展到实例分割和物体检测上，而且跟模型的改进可以互相补充，得到进一步提高！比如我们最近的另一篇工作，[ResNeSt](https://arxiv.org/abs/2004.08955)，从网络骨干架构的改进上出发，大幅提高了语义分割的精度，[霸榜ADE20K](https://zhuanlan.zhihu.com/p/136105870)，欢迎关注。接下来我们也会尝试合二为一，继续探索更通用的语义分割方法。
